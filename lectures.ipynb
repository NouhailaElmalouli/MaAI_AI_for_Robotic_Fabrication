{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CLASS_1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What good can ai in robotic fabrication do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make intelligent desicions\n",
    "2. adapt to changes\n",
    "3. optimise fabrication tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do we nned ai in robotic fabrication?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is usefull for the following applications:\n",
    "1. computer vision for real-time feedback\n",
    "2. Reinforcement learning for adaptive control\n",
    "3. ai driven toolpath optimization\n",
    "4. generative ai for design & fabrication\n",
    "5. HUman-Robot collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of uses of ai in robotic fabrication:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you prototype?**\n",
    "\n",
    "What-why-how needs to be answered<br>\n",
    "concepts of operationsdefinition<br>\n",
    "design<br>\n",
    "fabrication<br>\n",
    "\n",
    "**questions to answer:**\n",
    "\n",
    "where do we get the data?<br>\n",
    "how do we test? how do we simulate?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is suggested that we work to get the fabrication data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CLASS_2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reinforcement learning:\n",
    "\n",
    "1. it is all about trial and error\n",
    "2. instead of being told what to do, an agent learns by interacting with the environment\n",
    "3. inspired by desicion making and robotics.\n",
    "\n",
    "\n",
    "wokflow:\n",
    "\n",
    "\n",
    "use cases:\n",
    "\n",
    "HVAC Contorl\n",
    "urban planning\n",
    "autonomous construction robots\n",
    "floor plan optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinfeorcement Learning exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. paddle game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import spaces\n",
    "\n",
    "class CatchGameEnv(gym.Env):\n",
    "    def __init__(self, width=400, height=400):\n",
    "        super().__init__()\n",
    "        self.width, self.height = width, height\n",
    "        self.paddle_width = 80\n",
    "        self.paddle_x = (width - self.paddle_width) // 2\n",
    "        self.block_x = np.random.randint(0, width - self.paddle_width)\n",
    "        self.block_y = 0\n",
    "        self.block_speed = 5\n",
    "        self.done = False\n",
    "        \n",
    "        # Define action & observation spaces\n",
    "        self.action_space = spaces.Discrete(3)  # Move left, stay, move right\n",
    "        self.observation_space = spaces.Box(low=0, high=width, shape=(2,), dtype=np.int32)\n",
    "\n",
    "        # Initialize pygame\n",
    "        self.screen = pygame.display.set_mode((width, height))\n",
    "        pygame.display.set_caption(\"Catch the Falling Block\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.paddle_x = (self.width - self.paddle_width) // 2\n",
    "        self.block_x = np.random.randint(0, self.width - self.paddle_width)\n",
    "        self.block_y = 0\n",
    "        self.done = False\n",
    "        return np.array([self.paddle_x, self.block_x]), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:  # Move left\n",
    "            self.paddle_x = max(0, self.paddle_x - 20)\n",
    "        if action == 2:  # Move right\n",
    "            self.paddle_x = min(self.width - self.paddle_width, self.paddle_x + 20)\n",
    "\n",
    "        self.block_y += self.block_speed  # Block falls down\n",
    "\n",
    "        # Check if block reaches bottom\n",
    "        if self.block_y >= self.height - 20:\n",
    "            reward = 10 if abs(self.paddle_x - self.block_x) < 40 else -10\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        return np.array([self.paddle_x, self.block_x]), reward, self.done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0), (self.paddle_x, self.height - 20, self.paddle_width, 10))\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), (self.block_x, self.block_y, 20, 20))\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(30)\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "\n",
    "env = CatchGameEnv()\n",
    "\n",
    "q_table = np.zeros((400, 400, 3))  # Q-values for (paddle_x, block_x, action)\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        paddle_x, block_x = state\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[paddle_x, block_x])  # Exploit\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_paddle_x, next_block_x = next_state\n",
    "\n",
    "        # Q-learning update\n",
    "        q_table[paddle_x, block_x, action] = (1 - alpha) * q_table[paddle_x, block_x, action] + \\\n",
    "            alpha * (reward + gamma * np.max(q_table[next_paddle_x, next_block_x]))\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    paddle_x, block_x = state\n",
    "    action = np.argmax(q_table[paddle_x, block_x])  # Use trained policy\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Discretization settings\n",
    "num_bins = 10  # Fewer bins = faster learning, but less precision\n",
    "state_bins = [np.linspace(low, high, num_bins) for low, high in zip(env.observation_space.low, env.observation_space.high)]\n",
    "\n",
    "# Q-table initialization\n",
    "q_table = np.zeros([num_bins] * len(env.observation_space.low) + [env.action_space.n])\n",
    "\n",
    "def discretize_state(state):\n",
    "    \"\"\"Convert continuous state into discrete bins.\"\"\"\n",
    "    return tuple(np.digitize(state[i], state_bins[i]) - 1 for i in range(len(state)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_decay = 0.995  # Decay factor\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 50000  # Increase for better learning\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action (exploration-exploitation tradeoff)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit best known action\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state)\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * q_table[next_state][best_next_action])\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay exploration rate\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "state = discretize_state(state)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])  # Select best action\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    state = discretize_state(state)\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Slow down rendering for visibility\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floor plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "\n",
    "class FloorPlanEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(FloorPlanEnv, self).__init__()\n",
    "        self.grid_size = 5\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.grid_size, self.grid_size), dtype=np.int8)\n",
    "        self.action_space = spaces.Discrete(4)  # Move room: up, down, left, right\n",
    "        self.room_position = [2, 2]  # Start in the center\n",
    "\n",
    "    def reset(self):\n",
    "        self.room_position = [2, 2]\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0: self.room_position[0] = max(0, self.room_position[0] - 1)  # Up\n",
    "        if action == 1: self.room_position[0] = min(self.grid_size - 1, self.room_position[0] + 1)  # Down\n",
    "        if action == 2: self.room_position[1] = max(0, self.room_position[1] - 1)  # Left\n",
    "        if action == 3: self.room_position[1] = min(self.grid_size - 1, self.room_position[1] + 1)  # Right\n",
    "        \n",
    "        reward = -1  # Penalize unnecessary moves\n",
    "        if self.room_position == [4, 4]:  # Goal: reach optimal placement\n",
    "            reward = 10  \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.zeros((self.grid_size, self.grid_size), dtype=np.int8)\n",
    "        obs[self.room_position[0], self.room_position[1]] = 1\n",
    "        return obs\n",
    "\n",
    "env = FloorPlanEnv()\n",
    "\n",
    "q_table = np.zeros((5, 5, 4))  # Q-values for each (x, y, action)\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        x, y = env.room_position\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[x, y])  # Exploit\n",
    "        \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_x, next_y = env.room_position\n",
    "\n",
    "        # Q-learning update\n",
    "        q_table[x, y, action] = (1 - alpha) * q_table[x, y, action] + alpha * (reward + gamma * np.max(q_table[next_x, next_y]))\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    x, y = env.room_position\n",
    "    action = np.argmax(q_table[x, y])\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    print(f\"Room moved to: {env.room_position}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CLASS_3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PROJECT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BRAINSTORM AND CHOOSING THE TOPIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. trasnforming a physiscal object into a dynamic 3d geometry through the use of sensing tools.\n",
    "\n",
    "2. using ml approaches to make autonomous drones that allows them to detect and avoid obstacles.\n",
    "\n",
    "3. use ai to control the coordination of robots: <br>\n",
    "    3.1 use swarms to move objects. <br>\n",
    "    3.2 use swarm to get from point a to point b using coordinaton between the robot caabilities. <br>\n",
    "    3.3 use swarms to get robots to coordinate and build something architectural. <br>\n",
    "\n",
    "4. translating flat drawings into drawings in not planar surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawing image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI PROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROBOTIC ACTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boat thingy taskies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aside from the entrance fee and the tickets, everything else is covered.\n",
    "\n",
    "there will be a roadmap at some point \n",
    "\n",
    "we about to get informed about software tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 team focuses on identification of the elements \n",
    "\n",
    "the other team focuses \n",
    "\n",
    "\n",
    "\n",
    "navigation\n",
    "\n",
    "path planning \n",
    "\n",
    "collision \n",
    "\n",
    "localization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
